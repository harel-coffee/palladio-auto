%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional\else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\fi\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}

\usepackage{geometry}
\usepackage{multirow}
\usepackage{eqparbox}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\extrasenglish{\def\pageautorefname{page}}





\title{PALLADIO Documentation}
\date{May 29, 2017}
\release{2.0.3rc1}
\author{Matteo Barbieri \and Samuele Fiorini \and Federico Tomasi \and Annalisa Barla}
\newcommand{\sphinxlogo}{\sphinxincludegraphics{palladio_logo_npbg.png}\par}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


\sphinxstylestrong{PALLADIO} is a machine learning framework whose purpose is to provide robust and reproducible results when dealing with data where the signal to noise ratio is low.
It also provides tools to determine whether the dataset being analyzed contains any signal at all.
\sphinxstylestrong{PALLADIO} works by repeating the same experiment many times, each time resampling the learning and the test set so that the outcome is reliable as it is not determined by a \sphinxstyleemphasis{single} partition of the dataset. Besides, using permutation tests, it is possible to provide, to some extent, a measure of how reliable the results produced by an experiments are.
Since all experiments performed are independent, PALLADIO is designed so that it can exploit a cluster where it is available, in order to greatly reduce the amount of time required.

The final output of \sphinxstylestrong{PALLADIO} consists of several plots and text reports. The main ones are:
\begin{itemize}
\item {} 
A plot showing the absolute frequencies of features for both \sphinxstyleemphasis{regular} experiments and permutation tests. Another plot shows in more detail the selection frequency for the most frequently selected features (i.e., those above the \sphinxstyleemphasis{selection threshold} defined in the configuration file).

\item {} 
A plot showing the distribution of accuracies achieved by \sphinxstyleemphasis{regular} experiments and permutation tests.

\item {} 
Two text files listing the features together with their absolute selection frequency, one for regular experiments and the other for permutation tests.

\end{itemize}

See {\hyperref[\detokenize{tutorial:tutorial}]{\sphinxcrossref{\DUrole{std,std-ref}{Quick start tutorial}}}} for instructions on how to setup a cluster using the deployment script included in the \sphinxstylestrong{PALLADIO} distribution.


\chapter{User documentation}
\label{\detokenize{index:palladio-parallel-framework-for-model-selection}}\label{\detokenize{index:user-documentation}}

\section{Introduction}
\label{\detokenize{introduction:introduction}}\label{\detokenize{introduction::doc}}\label{\detokenize{introduction:id1}}
The issue of reproducibility of experiments is of paramount importance in scientific studies, as it influences the reliability of published findings. However when dealing with biological data, especially genomic data such as gene expression or SNP microarrays, it is not uncommon to have a very limited number of samples available, and these are usually represented by a huge number of measurements.

A common scenario is the so called \sphinxstyleemphasis{case-control study}: some quantities (e.g., gene expression levels, presence of alterations in key \sphinxstyleemphasis{loci} in the genome) are measured in a number of individuals who may be divided in two groups, or classes, depending whether they are affected by some kind of disease or not; the goal of the study is to find \sphinxstylestrong{which ones}, if any, among the possibly many measurements, or \sphinxstyleemphasis{features}, taken from the individuals (\sphinxstyleemphasis{samples}), can be used to define a \sphinxstyleemphasis{function} (sometimes the term \sphinxstyleemphasis{model} is used as well) able to \sphinxstyleemphasis{predict}, to some extent, to which \sphinxstyleemphasis{class} (in this case, a diseased individual or a healthy one) an individual belongs.

Machine Learning (ML) techniques work by \sphinxstyleemphasis{learning} such function using only \sphinxstyleemphasis{part} of the available samples (the \sphinxstyleemphasis{training set}), so that the remaining ones (the \sphinxstyleemphasis{test set}) can be used to determine how well the function is able to predict the class of \sphinxstylestrong{new} samples; this is done, roughly speaking, to ensure that the function is able to capture some real characteristics of the data and not simply fitting the training data, which is trivial.
This is referred to in ML literature as \sphinxstyleemphasis{binary classification scenario}.

In the aforementioned scenario, having only few samples available means that the learned function may be highly dependent on how the dataset was split; a common solution to this issue is to perform \sphinxstyleemphasis{K-fold cross validation} (KCV) which means splitting the dataset in \(K\) chunks and performing the experiment \(K\) times, each time leaving out a different chunk to be used as test set; this reduces the risk that the results are dependent on a particular split. The \(K\) parameter usually is chosen between 3 and 10, depending on the dataset.

This is the idea behind \sphinxhref{http://slipguru.disi.unige.it/Software/L1L2Signature/}{L1L2Signature} , a framework specifically designed with this issue in mind.
\sphinxcode{L1L2Signature} performs \sphinxstyleemphasis{feature selection} while learning the function, that is it tries to identify which ones among the available features are actually \sphinxstyleemphasis{relevant} for the problem, that is \sphinxstyleemphasis{which are actually used} in the learned function.
The output of \sphinxcode{L1L2Signature} consists of a \sphinxstyleemphasis{signature}, that is a list of relevant features, as well as a measure of \sphinxstyleemphasis{prediction accuracy}, that is the ratio of correctly classified samples in the test set, averaged over all splits.

There are however cases where it is hard to tell whether this procedure actually yielded a meaningful result: for instance, the fact that the accuracy measure is only \sphinxstyleemphasis{slightly} higher than chance can indicate two very different things:
\begin{itemize}
\item {} 
The available features can only describe the phenomenon to a limited extent.

\item {} 
There is actually no relationship between features and output class, and getting a result better than chance was just a matter of luck in the subdivision of the dataset.

\end{itemize}

In order to tackle this issue, \sphinxstylestrong{PALLADIO} repeats the experiment many times (\(\sim 100\)), each time using a different training and test set by randomly sampling from the whole original dataset (without replacement).
The experiment is also repeated the same number of times in a similar setting with a difference: in training sets, the labels are randomly shuffled, therefore destroying any connection between features and output class.

The output of this procedure is not a single value, possibly averaged, for the accuracy, but instead \sphinxstyleemphasis{two distributions of values} (one for each of the two settings described above) which, in case of datasets where the relationship between features and output class is at most faint, allows users to distinguish between the two scenarios mentioned above: in facts, if the available features are somehow connected with the outcome class, even weakly, then the two distributions will be  different enough to be distinguished; if on the other hand features and class are not related in any way, the two distributions will be indistinguishable, and it will be safe to draw that conclusion.


\section{The framework}
\label{\detokenize{framework:framework}}\label{\detokenize{framework::doc}}\label{\detokenize{framework:the-framework}}
A dataset consists of two things:
\begin{itemize}
\item {} 
An input matrix \(X \in \mathbb{R}^{n \times p}\) representing \(n\) samples each one described by \(p\) features; in the case of gene expression microarrays for instance each feature represents

\item {} 
An output vector \({\bf y}\) of length \(n\) whose elements are either a continuous value or a discrete label, describing some property of the samples. These may represent for example the levels of a given substance in the blood of an individual (continuous variable) or the \sphinxstyleemphasis{class} to which he or she belongs (for instance, someone affected by a given disease or a healthy control).

\end{itemize}

For the time being, we will only consider a specific instance of the latter case, where the number of classes is two: this is commonly referred to as \sphinxstyleemphasis{binary classification} scenario.

As previously explained, the core idea behind \sphinxstylestrong{PALLADIO} is to return, together with a list of significant features, not just a single value as an estimate for the prediction accuracy which can be achieved, but a distribution, so that it can be compared with the distribution obtained from experiments when the function is learned from data where the labels have been randomly shuffled (see {\hyperref[\detokenize{introduction:introduction}]{\sphinxcrossref{\DUrole{std,std-ref}{Introduction}}}}).


\subsection{Pipeline}
\label{\detokenize{framework:pipeline}}\label{\detokenize{framework:id1}}
Once the main script has been launched, the configuration file is read in order to retrieve all required information to run all the experiments of a \sphinxstylestrong{PALLADIO} \sphinxstyleemphasis{session}. These include:
\begin{itemize}
\item {} 
The location of \sphinxstylestrong{data} and \sphinxstylestrong{labels} files.

\item {} 
Experiment design parameters, such as the total number of experiments and the ratio of samples to be used for testing in each experiment.

\item {} 
Parameters specific to the chosen machine learning algorithm: for instance, for the \(\ell_1 \ell_2\) regularized algorithm, the values for the \(\tau\) and \(\lambda\) parameters.

\end{itemize}

A \sphinxstyleemphasis{session folder} is created within the folder containing the configuration file, in order to keep everything as contained as possible; data and labels file, together with the configuration file itself, are copied inside this folder. Then, experiments are distributed among the machines of the cluster; each machine will be assigned roughly the same number of jobs in order to balance the load.


\subsubsection{Experiments}
\label{\detokenize{framework:experiments}}
Each experiment is divided in several stages, as shown in \hyperref[\detokenize{framework:experiment-stages}]{Fig.\@ \ref{\detokenize{framework:experiment-stages}}}:
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{experiment_stages}.pdf}
\caption{The stages each experiment goes through.}\label{\detokenize{framework:experiment-stages}}\end{figure}


\paragraph{Dataset split and preprocessing}
\label{\detokenize{framework:dataset-split-and-preprocessing}}
In the very first stage, the dataset is split in \sphinxstylestrong{training} and \sphinxstylestrong{test} set, in a ratio determined by the corresponding parameter in the experiment configuration file; also, during this stage, any kind of data preprocessing (such as centering or normalization) is performed.


\paragraph{Model selection}
\label{\detokenize{framework:model-selection}}
Assuming that the chosen classifier requires some parameter to be specified (for instance the \(\ell_1\) and squared \(\ell_2\) penalities weights when using the \(\ell_1 \ell_2\) regularized least square algorithm), the \sphinxstylestrong{training} set is split in \(K\) chunks (the number \(K\) is also specified in the experiment configuration file) and K-fold cross validation is performed in order to choose the best parameters, that is those which lead to the model with the lowest cross validation error.


\paragraph{Model assessment}
\label{\detokenize{framework:model-assessment}}
Finally, the algorithm is trained using the parameters chosen in the previous step on the whole \sphinxstylestrong{training set}; the function obtained is then used to predict the labels of samples belonging to the \sphinxstylestrong{test set}, which have not been used so far in any way, so that the results of whole procedure are unbiased.

At the end of each experiment, results are stored in a \sphinxcode{.pkl} file inside a subfolder whose name will be of the form \sphinxcode{regular\_p\_P\_i\_I} for regular experiments and \sphinxcode{permutation\_p\_P\_i\_I} for experiments where the training labels have been randomly shuffled, where \sphinxcode{P} and \sphinxcode{I} the process number and within that process a counter which is incremented by one after each experiment.


\subsection{Analysis}
\label{\detokenize{framework:id2}}\label{\detokenize{framework:analysis}}
The analysis script simply reads the partial results in all experiment folders, consisting of
\begin{itemize}
\item {} 
A list of features

\item {} 
The predicted labels for the test set

\end{itemize}

With these it computes the accuracy achieved and then uses these elaborated results to produce a number of plots:

\hyperref[\detokenize{framework:manhattan-plot}]{Fig.\@ \ref{\detokenize{framework:manhattan-plot}}} shows the absolute feature selection frequency in both \sphinxstyleemphasis{regular} experiments and permutation tests; each tick on the horizontal axis represents a different feature, whose position on the vertical axis is the number of times it was selected in an experiment. Features are sorted based on the selection frequency relative to \sphinxstyleemphasis{regular} experiments; green dots are frequencies for \sphinxstyleemphasis{regular} experiments, red ones for permutation tests.
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{manhattan_plot}.pdf}
\caption{A manhattan plot showing the distribution of frequencies for both \sphinxstyleemphasis{regular} experiments and permutation tests.}\label{\detokenize{framework:manhattan-plot}}\end{figure}

\hyperref[\detokenize{framework:signature-frequencies}]{Fig.\@ \ref{\detokenize{framework:signature-frequencies}}} shows a detail of the frequeny of the top \(2 \times p_{\rm rel}\) selected features, where \(p_{\rm rel}\) is the number of features identified as \sphinxstyleemphasis{relevant} by the framework, i.e. those which have been selected enough times according to the selection threshold defined in the configuration file. Seeing the selection frequency of \sphinxstyleemphasis{relevant} features with respect to the selection frequency of those which have been rejected may help better interpret the obtained results.
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{signature_frequencies}.pdf}
\caption{A detail of the manhattan plot.}\label{\detokenize{framework:signature-frequencies}}\end{figure}

Finally, \hyperref[\detokenize{framework:permutation-acc-distribution}]{Fig.\@ \ref{\detokenize{framework:permutation-acc-distribution}}} shows the distribution of prediction accuracies (corrected for class imbalance) for \sphinxstyleemphasis{regular} experiments and permutation tests; this plot answer the questions:
\begin{itemize}
\item {} 
Is there any signal in the data being analyzed?

\item {} 
If yes, how much the model can describe it?

\end{itemize}

In the example figure, the two distributions are clearly different, and the green one (showing the accuracies of \sphinxstyleemphasis{regular} experiments) has a mean which is significantly higher than chance (50 \%). A p-value obtained with the Wilcoxon rank sum test is also present in this plot, indicating whether there is a significant difference between the two distributions.
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{permutation_acc_distribution}.pdf}
\caption{The distributions of accuracies for both \sphinxstyleemphasis{regular} experiments and permutation tests.}\label{\detokenize{framework:permutation-acc-distribution}}\end{figure}


\section{Quick start tutorial}
\label{\detokenize{tutorial::doc}}\label{\detokenize{tutorial:quick-start-tutorial}}\label{\detokenize{tutorial:tutorial}}
\sphinxstylestrong{PALLADIO} may be installed using standard Python tools (with
administrative or sudo permissions on GNU-Linux platforms):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install palladio

or

\PYGZdl{} easy\PYGZus{}install palladio
\end{sphinxVerbatim}

We strongly suggest to use \sphinxhref{https://www.continuum.io/downloads}{Anaconda} and create an environment for your experiments.


\subsection{Installation from sources}
\label{\detokenize{tutorial:installation-from-sources}}
If you like to manually install \sphinxstylestrong{PALLADIO}, download the .zip or .tar.gz archive
from \sphinxurl{http://slipguru.github.io/palladio/}. Then extract it and move into the root directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} unzip slipguru\PYGZhy{}palladio\PYGZhy{}\textbar{}release\textbar{}.zip
\PYGZdl{} cd palladio\PYGZhy{}\textbar{}release\textbar{}/
\end{sphinxVerbatim}

or:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} tar xvf slipguru\PYGZhy{}palladio\PYGZhy{}\textbar{}release\textbar{}.tar.gz
\PYGZdl{} cd palladio\PYGZhy{}\textbar{}release\textbar{}/
\end{sphinxVerbatim}

Otherwise you can clone our \sphinxhref{https://github.com/slipguru/palladio}{GitHub repository}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} git clone https://github.com/slipguru/palladio.git
\end{sphinxVerbatim}

From here, you can follow the standard Python installation step:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python setup.py install
\end{sphinxVerbatim}

After \sphinxstylestrong{PALLADIO} installation, you should have access to two scripts,
named with a common \sphinxcode{pd\_} prefix:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pd\PYGZus{}\PYGZlt{}TAB\PYGZgt{}
pd\PYGZus{}analysis.py    pd\PYGZus{}run.py
\end{sphinxVerbatim}

This tutorial assumes that you downloaded and extracted \sphinxstylestrong{PALLADIO}
source package which contains a \sphinxcode{palladio/config\_templates} directory with some data files (\sphinxcode{.npy} or \sphinxcode{.csv}) which will be used to show \sphinxstylestrong{PALLADIO} functionalities.

\sphinxstylestrong{PALLADIO} needs only 3 ingredients:
\begin{itemize}
\item {} 
\sphinxcode{n\_samples x n\_variables} input matrix

\item {} 
\sphinxcode{n\_samples x 1} labels vector

\item {} 
a \sphinxcode{configuration} file

\end{itemize}


\subsection{Cluster setup}
\label{\detokenize{tutorial:id1}}\label{\detokenize{tutorial:cluster-setup}}
Since all experiments performed during a run are independent from one another, \sphinxstylestrong{PALLADIO} has been designed specifically to work in a cluster environment.
It is fairly easy to prepare the cluster for the experiments: assuming a standard configuration for the nodes (a shared home folder and a python installation which includes standard libraries for scientific computation, namely \sphinxcode{numpy}, \sphinxcode{scipy} and \sphinxcode{sklearn}, as well as of course the \sphinxcode{mpi4py} library for the MPI infrastructure), it is sufficient to transfer on the cluster a folder containing the dataset (data matrix and labels) and the configuration file, and install \sphinxstylestrong{PALLADIO} itself following the instructions above.
\begin{quote}
\end{quote}


\subsection{Configuration File}
\label{\detokenize{tutorial:configuration-file}}\label{\detokenize{tutorial:configuration}}
\sphinxstylestrong{PALLADIO} configuration file is a standard Python script. It is
imported as a module, then all the code is executed. In this file the user defines all the parameters required to run a \sphinxstyleemphasis{session}, that is to perform all the experiments required to produce the final plots and reports.

In folder \sphinxcode{palladio/config\_templates} you will find an example of a typical configuration file.
Every configuration file has several sections which control different aspects of the procedure.

The code below contains all the information required to load the dataset which will be used in the experiments.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data\PYGZus{}path} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/gedm.csv}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{target\PYGZus{}path} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data/labels.csv}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} pandas.read\PYGZus{}csv options}
\PYG{n}{data\PYGZus{}loading\PYGZus{}options} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{delimiter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{header}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index\PYGZus{}col}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}
\PYG{p}{\PYGZcb{}}
\PYG{n}{target\PYGZus{}loading\PYGZus{}options} \PYG{o}{=} \PYG{n}{data\PYGZus{}loading\PYGZus{}options}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{datasets}\PYG{o}{.}\PYG{n}{load\PYGZus{}csv}\PYG{p}{(}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{dirname}\PYG{p}{(}\PYG{n}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{p}{,}\PYG{n}{data\PYGZus{}path}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{dirname}\PYG{p}{(}\PYG{n}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{p}{,}\PYG{n}{target\PYGZus{}path}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{data\PYGZus{}loading\PYGZus{}options}\PYG{o}{=}\PYG{n}{data\PYGZus{}loading\PYGZus{}options}\PYG{p}{,}
    \PYG{n}{target\PYGZus{}loading\PYGZus{}options}\PYG{o}{=}\PYG{n}{target\PYGZus{}loading\PYGZus{}options}\PYG{p}{,}
    \PYG{n}{samples\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{col}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{data}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data}\PYG{p}{,} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{target}
\PYG{n}{feature\PYGZus{}names} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{feature\PYGZus{}names}
\end{sphinxVerbatim}

The last two lines store the input data matrix \sphinxcode{data} and the labels vector \sphinxcode{labels} in two variables which will be accessible during the session. The names of the features are also saved at this point.
Notice how it is possible to load the dataset in any desired way, as long as \sphinxcode{data} ends up being a \(n \times d\) matrix and \sphinxcode{labels} a vector of \(n\) elements (both \sphinxcode{np.array}-like).

Next, we have the section containing settings relative to the session itself:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{session\PYGZus{}folder} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{palladio\PYGZus{}test\PYGZus{}session}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} The learning task, if None palladio tries to guess it}
\PYG{c+c1}{\PYGZsh{} [see sklearn.utils.multiclass.type\PYGZus{}of\PYGZus{}target]}
\PYG{n}{learning\PYGZus{}task} \PYG{o}{=} \PYG{n+nb+bp}{None}

\PYG{c+c1}{\PYGZsh{} The number of repetitions of \PYGZsq{}regular\PYGZsq{} experiments}
\PYG{n}{n\PYGZus{}splits\PYGZus{}regular} \PYG{o}{=} \PYG{l+m+mi}{50}

\PYG{c+c1}{\PYGZsh{} The number of repetitions of \PYGZsq{}permutation\PYGZsq{} experiments}
\PYG{n}{n\PYGZus{}splits\PYGZus{}permutation} \PYG{o}{=} \PYG{l+m+mi}{50}
\end{sphinxVerbatim}

The most important settings are the last two, namely \sphinxcode{n\_splits\_regular} and \sphinxcode{n\_splits\_permutation}, which control how many repetitions of \sphinxstyleemphasis{regular} and \sphinxstyleemphasis{permutations} experiments are performed.
Normally you'll want to perform the same number of experiments for the two \sphinxstyleemphasis{batches}, but there are cases in which for instance you may want to perform only one of the two batches: in that case you will want to set one of the two variables to be 0.

Finally, the section of the configuration file where the actual variable selection and learing algorithms (and their parameters) are chosen:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{RFE}\PYG{p}{(}\PYG{n}{LinearSVC}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hinge}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set the estimator to be a GridSearchCV}
\PYG{n}{param\PYGZus{}grid} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}features\PYGZus{}to\PYGZus{}select}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{estimator\PYGZus{}\PYGZus{}C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{logspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}

\PYG{n}{estimator} \PYG{o}{=} \PYG{n}{GridSearchCV}\PYG{p}{(}
  \PYG{n}{model}\PYG{p}{,}
  \PYG{n}{param\PYGZus{}grid}\PYG{o}{=}\PYG{n}{param\PYGZus{}grid}\PYG{p}{,}
  \PYG{n}{cv}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
  \PYG{n}{scoring}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
  \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set options for ModelAssessment}
\PYG{n}{ma\PYGZus{}options} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.25}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scoring}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}jobs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}splits}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{n\PYGZus{}splits\PYGZus{}regular}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

This is maybe the less intuitive part of the file.
Because of the way \sphinxstylestrong{PALLADIO} is designed, for all repetitions of the experiment a new learning and test set are generated by resampling without replacement from the whole dataset, then an estimator is used to fit the learning set. This is where that estimator (and its parameter) is defined.

Think about the \sphinxcode{estimator} variable as the \sphinxcode{sklearn}-compatible object (an estimator) which you would use to fit a training set, with the intent of validating it on a separate test set.

In this example we use a RFE algorithm (see \sphinxurl{http://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.RFE.html}) for variable selection, which internally uses a Linear SVM for classification.
Then we use a \sphinxcode{GridSearchCV} (\sphinxurl{http://scikit-learn.org/stable/modules/generated/sklearn.model\_selection.GridSearchCV.html}) object to wrap the RFE object, because we want to optimize the parameters for the RFE object itself, which are defined just above the declaration of the \sphinxcode{estimator} variable.

The dictionary \sphinxcode{ma\_options} define some more configuration options for the {\hyperref[\detokenize{api:palladio.model_assessment.ModelAssessment}]{\sphinxcrossref{\sphinxcode{ModelAssessment}}}} object, which is the one responsible for the outer iterations (the ones where the dataset is resampled);
the \sphinxcode{test\_size} key for instance determins the portion of data left aside for testing.


\subsection{Running the experiments}
\label{\detokenize{tutorial:running-experiments}}\label{\detokenize{tutorial:running-the-experiments}}
Parallel jobs are created by invoking the \sphinxcode{mpirun} command; the following syntax assumes that the \sphinxhref{https://www.open-mpi.org/}{OpenMPI} implementation of MPI has been chosen for the cluster, if this is not the case, please refer to the documentation of the implementation available on your cluster for the command line options corresponding to those specified here:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} mpirun \PYGZhy{}np N\PYGZus{}JOBS \PYGZhy{}\PYGZhy{}hostfile HOSTFILE pd\PYGZus{}run.py path/to/config.py
\end{sphinxVerbatim}

Here \sphinxcode{N\_JOBS} obviously determines how many parallel jobs will be spawned and distributed among all available nodes, while \sphinxcode{HOSTFILE} is a file listing the addresses or names of the available nodes.

Take into account that if optimized linear algebra libraries are present on the nodes (as it is safe to assume for most clusters) you should tune the number of jobs so that cores are optimally exploited: since those libraries already parallelize operations, it is useless to assign too many slots for each node.


\subsubsection{Running experiments on a single machine}
\label{\detokenize{tutorial:running-experiments-on-a-single-machine}}
It is possible to perform experiments using \sphinxstylestrong{PALLADIO} also on a single machine, without a cluster infrastructure. The command is similar to the previous one, it is sufficient to omit the first part, relative to the MPI infrastructure:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pd\PYGZus{}run.py path/to/config.py
\end{sphinxVerbatim}

\begin{sphinxadmonition}{warning}{Warning:}
Due to the great number of experiments which are performed, it might take a very long time for the whole procedure to complete; this option is therefore deprecated unless the dataset is very small (no more than 100 samples and no more than 100 features).
\end{sphinxadmonition}


\subsection{Results analysis}
\label{\detokenize{tutorial:id2}}\label{\detokenize{tutorial:results-analysis}}
The \sphinxcode{pd\_analysis.py} script reads the results from all experiments and produces several plots and text files. The syntax is the following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pd\PYGZus{}analysis.py path/to/results\PYGZus{}dir
\end{sphinxVerbatim}

See {\hyperref[\detokenize{framework:analysis}]{\sphinxcrossref{\DUrole{std,std-ref}{Analysis}}}} for further details on the output of the analysis.


\section{API}
\label{\detokenize{api:api}}\label{\detokenize{api::doc}}\label{\detokenize{api:id1}}

\subsection{Pipeline utilities}
\label{\detokenize{api:pipeline-utilities}}\label{\detokenize{api:module-palladio.model_assessment}}\index{palladio.model\_assessment (module)}
Nested Cross-Validation for scikit-learn using MPI.

This package provides nested cross-validation similar to scikit-learn's
GridSearchCV but uses the Message Passing Interface (MPI)
for parallel computing.
\index{ModelAssessment (class in palladio.model\_assessment)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.model_assessment.ModelAssessment}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{palladio.model\_assessment.}\sphinxbfcode{ModelAssessment}}{\emph{estimator}, \emph{cv=None}, \emph{scoring=None}, \emph{fit\_params=None}, \emph{multi\_output=False}, \emph{shuffle\_y=False}, \emph{n\_jobs=1}, \emph{n\_splits=10}, \emph{test\_size=0.1}, \emph{train\_size=None}, \emph{random\_state=None}, \emph{groups=None}, \emph{experiments\_folder=None}, \emph{verbose=False}}{}
Cross-validation with nested parameter search for each training fold.

The data is first split into \sphinxcode{cv} train and test sets. For each training
set a grid search over the specified set of parameters is performed
(inner cross-validation). The set of parameters that achieved the highest
average score across all inner folds is used to re-fit a model on the
entire training set of the outer cross-validation loop. Finally, results on
the test set of the outer loop are reported.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{estimator} : object type that implements the \sphinxquotedblleft{}fit\sphinxquotedblright{} and \sphinxquotedblleft{}predict\sphinxquotedblright{} methods
\begin{quote}

A object of that type is instantiated for each grid point.
\end{quote}

\sphinxstylestrong{cv} : integer or cross-validation generator, optional, default: 3
\begin{quote}

If an integer is passed, it is the number of folds.
Specific cross-validation objects can be passed, see
sklearn.cross\_validation module for the list of possible objects
\end{quote}

\sphinxstylestrong{scoring} : string, callable or None, optional, default: None
\begin{quote}

A string (see model evaluation documentation) or
a scorer callable object / function with signature
\sphinxcode{scorer(estimator, X, y)}.
See sklearn.metrics.get\_scorer for details.
\end{quote}

\sphinxstylestrong{fit\_params} : dict, optional, default: None
\begin{quote}

Parameters to pass to the fit method.
\end{quote}

\sphinxstylestrong{multi\_output} : boolean, default: False
\begin{quote}

Allow multi-output y, as for multivariate regression.
\end{quote}

\sphinxstylestrong{shuffle\_y} : bool, optional, default=False
\begin{quote}

When True, the object is used to perform permutation test.
\end{quote}

\sphinxstylestrong{n\_jobs} : int, optional, default: 1
\begin{quote}
\begin{quote}

The number of jobs to use for the computation. This works by computing
each of the Monte Carlo runs in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is
used at all, which is useful for debugging. Ignored when using MPI.
\end{quote}
\begin{description}
\item[{n\_splits: int, optional, default: 10}] \leavevmode
The number of cross-validation splits (folds/iterations).

\end{description}
\end{quote}

\sphinxstylestrong{test\_size} : float (default 0.1), int, or None
\begin{quote}

If float, should be between 0.0 and 1.0 and represent the
proportion of the dataset to include in the test split. If
int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size.
\end{quote}

\sphinxstylestrong{train\_size} : float, int, or None (default is None)
\begin{quote}
\begin{quote}

If float, should be between 0.0 and 1.0 and represent the
proportion of the dataset to include in the train split. If
int, represents the absolute number of train samples. If None,
the value is automatically set to the complement of the test size.
\end{quote}
\begin{description}
\item[{random\_state}] \leavevmode{[}int or RandomState, optional, default: None{]}
Pseudo-random number generator state used for random sampling.

\item[{groups}] \leavevmode{[}array-like, with shape (n\_samples,), optional, default: None{]}
Group labels for the samples used while splitting the dataset into
train/test set.

\end{description}
\end{quote}

\sphinxstylestrong{experiments\_folder} : string, optional, default: None
\begin{quote}

The path to the folder used to save the results.
\end{quote}

\sphinxstylestrong{verbose} : bool, optional, default: False
\begin{quote}

Print debug messages.
\end{quote}

\end{description}\end{quote}
\paragraph{Attributes}

\noindent\begin{tabular}{|*{3}{p{\dimexpr(\linewidth-\arrayrulewidth)/3-2\tabcolsep-\arrayrulewidth\relax}|}}
\hline

{\color{red}\bfseries{}scorer\_}
&
function
&
Scorer function used on the held out data to choose the best
parameters for the model.
\\
\hline
{\color{red}\bfseries{}cv\_results\_}
&
dictionary
&
Result of the fit. The dictionary is pandas.DataFrame-able. Each row is
the results of an external split.
Columns are:
`split\_i', `learn\_score', `test\_score', `{\color{red}\bfseries{}cv\_results\_}`, `ytr\_pred',
`yts\_pred', `test\_index', `train\_index', `estimator'

Example:
\textgreater{}\textgreater{}\textgreater{} pd.DataFrame({\color{red}\bfseries{}cv\_results\_})
split\_i \textbar{} learn\_score \textbar{} test\_score \textbar{} {\color{red}\bfseries{}cv\_results\_}         \textbar{} ...
\begin{quote}

0 \textbar{}       0.987 \textbar{}      0.876 \textbar{} \{\textless{}internal splits\textgreater{}\} \textbar{} ...
1 \textbar{}       0.846 \textbar{}      0.739 \textbar{} \{\textless{}internal splits\textgreater{}\} \textbar{} ...
2 \textbar{}       0.956 \textbar{}      0.630 \textbar{} \{\textless{}internal splits\textgreater{}\} \textbar{} ...
3 \textbar{}       0.964 \textbar{}      0.835 \textbar{} \{\textless{}internal splits\textgreater{}\} \textbar{} ...
\end{quote}
\\
\hline\end{tabular}

\index{fit() (palladio.model\_assessment.ModelAssessment method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.model_assessment.ModelAssessment.fit}}\pysiglinewithargsret{\sphinxbfcode{fit}}{\emph{X}, \emph{y}}{}
Fit the model to the training data.

\end{fulllineitems}


\end{fulllineitems}



\subsection{Extra tools}
\label{\detokenize{api:module-palladio.utils}}\label{\detokenize{api:extra-tools}}\index{palladio.utils (module)}
Utilities functions and classes.
\index{save\_signature() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.save_signature}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{save\_signature}}{\emph{filename}, \emph{selected}, \emph{threshold=0.75}}{}
Save signature summary.

\end{fulllineitems}

\index{retrieve\_features() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.retrieve_features}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{retrieve\_features}}{\emph{best\_estimator}}{}
Retrieve selected features from any estimator.

In case it has the `get\_support' method, use it.
Else, if it has a `{\color{red}\bfseries{}coef\_}` attribute, assume it's a linear model and the
features correspond to the indices of the coefficients != 0

\end{fulllineitems}

\index{get\_selected\_list() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.get_selected_list}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{get\_selected\_list}}{\emph{grid\_search}, \emph{vs\_analysis=True}}{}
Retrieve the list of selected features.

Retrieves the list of selected features automatically identifying the
type of object
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxstylestrong{index} : nunmpy.array
\begin{quote}

The indices of the selected features
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{build\_cv\_results() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.build_cv_results}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{build\_cv\_results}}{\emph{dictionary}, \emph{**results}}{}
Function to build final {\color{red}\bfseries{}cv\_results\_} dictionary with partial results.

\end{fulllineitems}

\index{signatures() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.signatures}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{signatures}}{\emph{splits\_results}, \emph{frequency\_threshold=0.0}}{}
Return (almost) nested signatures for each correlation value.

The function returns 3 lists where each item refers to a signature
(for increasing value of linear correlation).
Each signature is orderer from the most to the least selected variable
across KCV splits results.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{splits\_results} : iterable
\begin{quote}

List of results from L1L2Py module, one for each external split.
\end{quote}

\sphinxstylestrong{frequency\_threshold} : float
\begin{quote}

Only the variables selected more (or equal) than this threshold are
included into the signature.
\end{quote}

\item[{Returns}] \leavevmode
\sphinxstylestrong{sign\_totals} : list of \sphinxcode{numpy.ndarray}.
\begin{quote}

Counts the number of times each variable in the signature is selected.
\end{quote}

\sphinxstylestrong{sign\_freqs} : list of \sphinxcode{numpy.ndarray}.
\begin{quote}

Frequencies calculated from \sphinxcode{sign\_totals}.
\end{quote}

\sphinxstylestrong{sign\_idxs} : list of \sphinxcode{numpy.ndarray}.
\begin{quote}

Indexes of the signatures variables .
\end{quote}

\end{description}\end{quote}
\paragraph{Examples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{palladio}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k}{import} \PYG{n}{signatures}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{splits\PYGZus{}results} \PYG{o}{=} \PYG{p}{[}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{selected\PYGZus{}list}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{p}{[}\PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{]}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{g+gp}{... }                  \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{selected\PYGZus{}list}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{p}{[}\PYG{k+kc}{True}\PYG{p}{,} \PYG{k+kc}{False}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{k+kc}{False}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{]}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{sign\PYGZus{}totals}\PYG{p}{,} \PYG{n}{sign\PYGZus{}freqs}\PYG{p}{,} \PYG{n}{sign\PYGZus{}idxs} \PYG{o}{=} \PYG{n}{signatures}\PYG{p}{(}\PYG{n}{splits\PYGZus{}results}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print} \PYG{n}{sign\PYGZus{}totals}
\PYG{g+go}{[array([ 2.,  0.]), array([ 2.,  1.])]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print} \PYG{n}{sign\PYGZus{}freqs}
\PYG{g+go}{[array([ 1.,  0.]), array([ 1. ,  0.5])]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print} \PYG{n}{sign\PYGZus{}idxs}
\PYG{g+go}{[array([0, 1]), array([1, 0])]}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{selection\_summary() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.selection_summary}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{selection\_summary}}{\emph{splits\_results}}{}
Count how many times each variables was selected.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{splits\_results} : iterable
\begin{quote}

List of results from L1L2Py module, one for each external split.
\end{quote}

\item[{Returns}] \leavevmode
\sphinxstylestrong{summary} : \sphinxcode{numpy.ndarray}
\begin{quote}

Selection summary. \sphinxcode{\# mu\_values X \# variables} matrix.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{confusion\_matrix() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.confusion_matrix}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{confusion\_matrix}}{\emph{labels}, \emph{predictions}}{}
Calculate a confusion matrix.

From given real and predicted labels, the function calculated
a confusion matrix as a double nested dictionary.
The external one contains two keys, \sphinxcode{'T'} and \sphinxcode{'F'}.
Both internal dictionaries
contain a key for each class label. Then the \sphinxcode{{[}'T'{]}{[}'C1'{]}} entry counts
the number of correctly predicted \sphinxcode{'C1'} labels,
while \sphinxcode{{[}'F'{]}{[}'C2'{]}} the incorrectly predicted \sphinxcode{'C2'} labels.

Note that each external dictionary correspond to a confusion
matrix diagonal and the function works only on two-class labels.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{labels} : iterable
\begin{quote}

Real labels.
\end{quote}

\sphinxstylestrong{predictions} : iterable
\begin{quote}

Predicted labels.
\end{quote}

\item[{Returns}] \leavevmode
\sphinxstylestrong{cm} : dict
\begin{quote}

Dictionary containing the confusion matrix values.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{classification\_measures() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.classification_measures}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{classification\_measures}}{\emph{confusion\_matrix}, \emph{positive\_label=None}}{}
Calculate some classification measures.

Measures are calculated from a given confusion matrix
(see {\hyperref[\detokenize{api:palladio.utils.confusion_matrix}]{\sphinxcrossref{\sphinxcode{confusion\_matrix()}}}} for a detailed description of the
required structure).

The \sphinxcode{positive\_label} arguments allows to specify what label has to be
considered the positive class. This is needed to calculate some
measures like F-measure and set some aliases (e.g. precision and recall
are respectively the `predictive value' and the `true rate' for the
positive class).

If \sphinxcode{positive\_label} is None, the resulting dictionary will not
contain all the measures. Assuming to have to classes `C1' and `C2',
and to indicate `C1' as the positive (P) class, the function returns a
dictionary with the following structure:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{predictive\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} TP / (TP + FP)}
           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{true\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} TP / (TP + FN)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{predictive\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} TN / (TN + FN)}
           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{true\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} TN / (TN + FP)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}          \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} (TP + TN) / (TP + FP + FN + TN)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{balanced\PYGZus{}accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} 0.5 * ( (TP / (TP + FN)) +}
                                    \PYG{c+c1}{\PYGZsh{}         (TN / (TN + FP)) )}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MCC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}               \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} ( (TP * TN) \PYGZhy{} (FP * FN) ) /}
                                    \PYG{c+c1}{\PYGZsh{} sqrt( (TP + FP) * (TP + FN) *}
                                    \PYG{c+c1}{\PYGZsh{}       (TN + FP) * (TN + FN) )}

    \PYG{c+c1}{\PYGZsh{} Following, only with positive\PYGZus{}labels != None}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sensitivity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}       \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} P true rate: TP / (TP + FN)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{specificity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}       \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} N true rate: TN / (TN + FP)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}         \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} P predictive value: TP / (TP + FP)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{recall}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}            \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} P true rate: TP / (TP + FN)}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F\PYGZus{}measure}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}         \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}         \PYG{c+c1}{\PYGZsh{} 2. * ( (Precision * Recall ) /}
                                    \PYG{c+c1}{\PYGZsh{}        (Precision + Recall) )}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{confusion\_matrix} : dict
\begin{quote}

Confusion matrix (as the one returned by {\hyperref[\detokenize{api:palladio.utils.confusion_matrix}]{\sphinxcrossref{\sphinxcode{confusion\_matrix()}}}}).
\end{quote}

\sphinxstylestrong{positive\_label} : str
\begin{quote}

Positive class label.
\end{quote}

\item[{Returns}] \leavevmode
\sphinxstylestrong{summary} : dict
\begin{quote}

Dictionary containing calculated measures.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_module\_defaults() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.set_module_defaults}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{set\_module\_defaults}}{\emph{module}, \emph{dictionary}}{}
Set default variables of a module, given a dictionary.

Used after the loading of the configuration file to set some defaults.

\end{fulllineitems}

\index{sec\_to\_timestring() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.sec_to_timestring}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{sec\_to\_timestring}}{\emph{seconds}}{}
Transform seconds into a formatted time string.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstylestrong{seconds} : int
\begin{quote}

Seconds to be transformed.
\end{quote}

\sphinxstylestrong{Returns} :

\sphinxstylestrong{-----------} :

\sphinxstylestrong{time} : string
\begin{quote}

A well formatted time string.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{safe\_run() (in module palladio.utils)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api:palladio.utils.safe_run}}\pysiglinewithargsret{\sphinxcode{palladio.utils.}\sphinxbfcode{safe\_run}}{\emph{function}}{}
Decorator that tries to run a function and prints an error when fails.

\end{fulllineitems}



\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{p}
\item {\sphinxstyleindexentry{palladio.model\_assessment}}\sphinxstyleindexpageref{api:\detokenize{module-palladio.model_assessment}}
\item {\sphinxstyleindexentry{palladio.utils}}\sphinxstyleindexpageref{api:\detokenize{module-palladio.utils}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}